{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import clip\n",
    "import clip.simple_tokenizer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_path = 'save/testset_output'\n",
    "file_list = os.listdir(test_output_path)\n",
    "file_list = [f for f in file_list if f.endswith('.pkl')]\n",
    "file_list = sorted(file_list)\n",
    "KEYWORD = 'move'\n",
    "top_tokens = []\n",
    "for file_name in file_list:\n",
    "    print(file_name)\n",
    "    with open(os.path.join(test_output_path, file_name), 'rb') as f:\n",
    "        batch_output = pickle.load(f)\n",
    "    bs = batch_output['motion'].shape[0]\n",
    "    for b in range(bs):\n",
    "        n_frames = batch_output['lengths'][b]\n",
    "        attn_value = batch_output['attention_values'][0][b]\n",
    "        attn_mask = batch_output['attention_masks'][0][b]\n",
    "        attn_value = attn_value[:,:,attn_mask]\n",
    "        attn_value = attn_value[:,:,1:(attn_value.shape[-1]-1)]\n",
    "        attn_value = attn_value/attn_value.sum(axis=-1)[...,None]\n",
    "        text_input = batch_output['text'][b]\n",
    "        if KEYWORD not in text_input:\n",
    "            continue\n",
    "        text_id = clip.tokenize(text_input, truncate=True)[:,attn_mask][-1].tolist()\n",
    "        text_token = [clip.clip._tokenizer.decoder[token_id].replace('</w>',\"\") for token_id in text_id][1:attn_value.shape[-1]+1]\n",
    "\n",
    "        attn_value = attn_value.mean(axis=0).mean(axis=0)\n",
    "        top_token_id = numpy.argsort(attn_value)[::-1][:10]\n",
    "        count = 0 \n",
    "        for i in top_token_id:\n",
    "            if text_token[i].lower() not in ['person', 'man', 'someone']:\n",
    "                if count == 5:\n",
    "                    break\n",
    "                count += 1\n",
    "                top_tokens.append(text_token[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_token_sentence = ' '.join((top_tokens))\n",
    "import wordcloud\n",
    "# create wordcloud object with high dpi, white background, and max words\n",
    "wc = wordcloud.WordCloud(width=800, height=800, background_color='white', max_words=500, min_font_size=10)\n",
    "wc.generate(top_token_sentence)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip.simple_tokenizer\n",
    "\n",
    "num_instances = attn_dict['attn_values'].shape[0]\n",
    "bs = attn_dict['attn_values'].shape[2]\n",
    "top_tokens = []\n",
    "\n",
    "for b in range(bs):\n",
    "    n_frames = result_dict['lengths'][instance_id]\n",
    "    attn_value = attn_dict['attn_values'][instance_id][0][b]\n",
    "    attn_mask = attn_dict['attn_masks'][instance_id][-1][b]\n",
    "    attn_value = attn_value[:,:,attn_mask]\n",
    "    attn_value = attn_value[:,:,1:(attn_value.shape[-1]-1)]\n",
    "    attn_value = attn_value/attn_value.sum(axis=-1)[...,None]\n",
    "\n",
    "    text_input = result_dict['text'][instance_id*bs+b]\n",
    "    text_id = clip.tokenize(text_input)[:,attn_mask][0].tolist()\n",
    "    text_token = [clip.clip._tokenizer.decoder[token_id].replace('</w>',\"\") for token_id in text_id][1:attn_value.shape[-1]+1]\n",
    "\n",
    "    attn_value = attn_value.mean(axis=0).mean(axis=0)\n",
    "    top_token_id = numpy.argsort(attn_value)[::-1][:10]\n",
    "    count = 0 \n",
    "    for i in top_token_id:\n",
    "        if text_token[i].lower() not in ['person', 'man', 'someone']:\n",
    "            if count == 5:\n",
    "                break\n",
    "            count += 1\n",
    "            top_tokens.append(text_token[i])\n",
    "\n",
    "        \n",
    "top_token_sentence = ' '.join(top_tokens)\n",
    "import wordcloud\n",
    "# create wordcloud object with high dpi, white background, and max words\n",
    "wc = wordcloud.WordCloud(width=800, height=800, background_color='white', max_words=200, min_font_size=10)\n",
    "wc.generate(top_token_sentence)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = 'save/testset_output/MDM_CLIP_ATTN_batch_1.pkl'\n",
    "\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    outputs = pickle.load(f)\n",
    "    \n",
    "result_dict = outputs\n",
    "attn_dict = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip.simple_tokenizer\n",
    "\n",
    "num_instances = attn_dict['attention_values'].shape[0]\n",
    "bs = attn_dict['attention_values'].shape[1]\n",
    "\n",
    "\n",
    "for b in range(bs):\n",
    "    n_frames = result_dict['lengths'][b]\n",
    "    attn_value = attn_dict['attention_values'].mean(axis=0)[b]\n",
    "    attn_mask = attn_dict['attention_masks'][-1][b]\n",
    "    attn_value = attn_value[:,:,attn_mask]\n",
    "    attn_value = attn_value[:,:,1:(attn_value.shape[-1]-1)]\n",
    "    attn_value = attn_value/attn_value.sum(axis=-1)[...,None]\n",
    "\n",
    "    text_input = result_dict['text'][b]\n",
    "    text_id = clip.tokenize(text_input)[:,attn_mask][0].tolist()\n",
    "    text_token = [clip.clip._tokenizer.decoder[token_id].replace('</w>',\"\") for token_id in text_id][1:attn_value.shape[-1]+1]\n",
    "\n",
    "    # plot attention by head\n",
    "    num_head = attn_value.shape[0]\n",
    "    fig, axs = plt.subplots(1, num_head+1, figsize=(25, 10))\n",
    "    for h in range(num_head):\n",
    "        axs[h].imshow(attn_value[h].T, cmap='hot', interpolation='nearest')\n",
    "        # use the text_token as x-axis\n",
    "        axs[h].set_yticks(numpy.arange(len(text_token)))\n",
    "        axs[h].set_yticklabels(text_token, rotation=0)\n",
    "        # make each subplot wider\n",
    "        axs[h].set_aspect('auto')\n",
    "        axs[h].set_title(f'head {h}')\n",
    "    # plot the average attention\n",
    "    axs[num_head].imshow(attn_value.mean(axis=0).T, cmap='hot', interpolation='nearest')\n",
    "    axs[num_head].set_yticks(numpy.arange(len(text_token)))\n",
    "    axs[num_head].set_yticklabels(text_token, rotation=0)\n",
    "    axs[num_head].set_aspect('auto')\n",
    "    axs[num_head].set_title(f'average')\n",
    "    \n",
    "    plt.show()\n",
    "    # save_path = os.path.join(path, f'attn_rep{instance_id}_b{b}.png')\n",
    "    # fig.savefig(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for individual prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'save/Best_MDM_V2/samples_Best_MDM_V2_000100000_seed10_a_person_wave_his_arm,_and_then_sit_down'\n",
    "\n",
    "result_path = os.path.join(path, 'results.npy')\n",
    "attn_path = os.path.join(path, 'results_attn.npy')\n",
    "\n",
    "result_dict = numpy.load(result_path, allow_pickle=True).item()\n",
    "attn_dict = numpy.load(attn_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attention for key frames [0, 20, 40, 60, 80, 100]\n",
    "# x axis: text token\n",
    "# y axis: head 1, 2, 3, 4, average\n",
    "\n",
    "num_instances = attn_dict['attn_values'].shape[0]\n",
    "bs = attn_dict['attn_values'].shape[2]\n",
    "\n",
    "key_frames = [0, 20, 40, 60, 80, 100]\n",
    "\n",
    "for instance_id in range(num_instances):\n",
    "    for b in range(bs):\n",
    "        n_frames = result_dict['lengths'][instance_id]\n",
    "        attn_value = attn_dict['attn_values'][instance_id][0][b]\n",
    "        attn_mask = attn_dict['attn_masks'][instance_id][-1][b]\n",
    "        attn_value = attn_value[:,:,attn_mask]\n",
    "        attn_value = attn_value[:,:,1:(attn_value.shape[-1]-1)] # num_head x num_frame x num_token\n",
    "        attn_value = attn_value/attn_value.sum(axis=-1)[...,None]\n",
    "\n",
    "\n",
    "        text_input = result_dict['text'][instance_id*bs+b]\n",
    "        text_id = clip.tokenize(text_input)[:,attn_mask][0].tolist()\n",
    "        text_token = [clip.clip._tokenizer.decoder[token_id].replace('</w>',\"\") for token_id in text_id][1:attn_value.shape[-1]+1]\n",
    "\n",
    "        # plot attention by frames\n",
    "        attn_value = numpy.concatenate([attn_value, attn_value.mean(axis=0)[None]], axis=0)\n",
    "        fig, axs = plt.subplots(1, len(key_frames), figsize=(45, 5))\n",
    "        for i, frame in enumerate(key_frames):\n",
    "            axs[i].imshow(attn_value[:,frame,], cmap='hot_r', interpolation='nearest')\n",
    "            axs[i].set_xticks(numpy.arange(len(text_token)))\n",
    "            axs[i].set_xticklabels(text_token, rotation=45, fontsize=20)\n",
    "            axs[i].set_yticks(numpy.arange(attn_value.shape[0]))\n",
    "            if i == 0:\n",
    "                axs[i].set_yticklabels(['Head 1', 'Head 2', 'Head 3', 'Head 4', 'Average'], rotation=0, fontsize=15)\n",
    "            # axs[i].set_yticklabels(['Head 1', 'Head 2', 'Head 3', 'Head 4', 'Average'], rotation=0, fontsize=15)\n",
    "            axs[i].set_aspect('auto')\n",
    "            axs[i].set_title(f'Frame {frame}', fontsize=20)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip.simple_tokenizer\n",
    "\n",
    "num_instances = attn_dict['attn_values'].shape[0]\n",
    "bs = attn_dict['attn_values'].shape[2]\n",
    "\n",
    "# instance_id = 0\n",
    "for instance_id in range(num_instances):\n",
    "    for b in range(bs):\n",
    "        n_frames = result_dict['lengths'][instance_id]\n",
    "        attn_value = attn_dict['attn_values'][instance_id][0][b]\n",
    "        attn_mask = attn_dict['attn_masks'][instance_id][-1][b]\n",
    "        attn_value = attn_value[:,:,attn_mask]\n",
    "        attn_value = attn_value[:,:,1:(attn_value.shape[-1]-1)]\n",
    "        attn_value = attn_value/attn_value.sum(axis=-1)[...,None]\n",
    "\n",
    "        text_input = result_dict['text'][instance_id*bs+b]\n",
    "        text_id = clip.tokenize(text_input)[:,attn_mask][0].tolist()\n",
    "        text_token = [clip.clip._tokenizer.decoder[token_id].replace('</w>',\"\") for token_id in text_id][1:attn_value.shape[-1]+1]\n",
    "\n",
    "        # plot attention by head\n",
    "        num_head = attn_value.shape[0]\n",
    "        fig, axs = plt.subplots(1, num_head+1, figsize=(25, 5))\n",
    "        for h in range(num_head):\n",
    "            axs[h].imshow(attn_value[h], cmap='hot', interpolation='nearest')\n",
    "            # use the text_token as x-axis\n",
    "            axs[h].set_xticks(numpy.arange(len(text_token)))\n",
    "            axs[h].set_xticklabels(text_token, rotation=45)\n",
    "            # y-axis is the frame index (every 20 frames)\n",
    "            axs[h].set_yticklabels(numpy.arange(-20, n_frames, 20))\n",
    "            # make each subplot wider\n",
    "            axs[h].set_aspect('auto')\n",
    "            axs[h].set_title(f'head {h}')\n",
    "        # plot the average attention across heads\n",
    "        axs[num_head].imshow(attn_value.mean(axis=0), cmap='hot', interpolation='nearest')\n",
    "        axs[num_head].set_xticks(numpy.arange(len(text_token)))\n",
    "        axs[num_head].set_xticklabels(text_token, rotation=45)\n",
    "        # y-axis is the frame index (every 20 frames)\n",
    "        axs[num_head].set_yticklabels(numpy.arange(-20, n_frames, 20))\n",
    "        axs[num_head].set_aspect('auto')\n",
    "        axs[num_head].set_title(f'average')       \n",
    "        \n",
    "        plt.show()\n",
    "        save_path = os.path.join(path, f'attn_rep{instance_id}_b{b}.png')\n",
    "        fig.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip.simple_tokenizer\n",
    "\n",
    "num_instances = attn_dict['attn_values'].shape[0]\n",
    "bs = attn_dict['attn_values'].shape[2]\n",
    "top_tokens = []\n",
    "\n",
    "# instance_id = 0\n",
    "for instance_id in range(num_instances):\n",
    "    for b in range(bs):\n",
    "        n_frames = result_dict['lengths'][instance_id]\n",
    "        attn_value = attn_dict['attn_values'][instance_id][0][b]\n",
    "        attn_mask = attn_dict['attn_masks'][instance_id][-1][b]\n",
    "        attn_value = attn_value[:,:,attn_mask]\n",
    "        attn_value = attn_value[:,:,1:(attn_value.shape[-1]-1)]\n",
    "        attn_value = attn_value/attn_value.sum(axis=-1)[...,None]\n",
    "\n",
    "        text_input = result_dict['text'][instance_id*bs+b]\n",
    "        text_id = clip.tokenize(text_input)[:,attn_mask][0].tolist()\n",
    "        text_token = [clip.clip._tokenizer.decoder[token_id].replace('</w>',\"\") for token_id in text_id][1:attn_value.shape[-1]+1]\n",
    "\n",
    "        attn_value = attn_value.mean(axis=0).mean(axis=0)\n",
    "        top_token_id = numpy.argsort(attn_value)[::-1][:10]\n",
    "        count = 0 \n",
    "        for i in top_token_id:\n",
    "            if text_token[i].lower() not in ['person', 'man', 'someone']:\n",
    "                if count == 5:\n",
    "                    break\n",
    "                count += 1\n",
    "                top_tokens.append(text_token[i])\n",
    "\n",
    "        \n",
    "top_token_sentence = ' '.join(top_tokens)\n",
    "import wordcloud\n",
    "# create wordcloud object with high dpi, white background, and max words\n",
    "wc = wordcloud.WordCloud(width=800, height=800, background_color='white', max_words=200, min_font_size=10)\n",
    "wc.generate(top_token_sentence)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PriorMDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
