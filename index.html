<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We propose CASIM, a composite-aware semantic injection mechanism for text to motion generation that exhibits stronger text-motion correspondence and better generalizability."/>
  <meta property="og:title" content="CASIM: Composite Aware Semantic Injection for Text to Motion Generation"/>
  <meta property="og:description" content="We propose CASIM, a composite-aware semantic injection mechanism for text to motion generation that exhibits stronger text-motion correspondence and better generalizability."/>
  <meta property="og:url" content="https://cjerry1243.github.io/casim_t2m/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/TeaserImage.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="CASIM: Composite Aware Semantic Injection for Text to Motion Generation">
  <meta name="twitter:description" content="We propose CASIM, a composite-aware semantic injection mechanism for text to motion generation that exhibits stronger text-motion correspondence and better generalizability.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/TeaserImage.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="human motion generation, text to motion, semantic injection, composite aware text to motion generation, casim, t2m">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CASIM: Composite Aware Semantic Injection for Text to Motion Generation</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CASIM: Composite Aware Semantic Injection for Text to Motion Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                  <span class="author-block">
                    <a href="https://sites.google.com/view/chejuichang/" target="_blank">Che-Jui Chang*</a><sup>1,2</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/qingze-tony-liu-6b0579195/" target="_blank">Qingze Tony Liu*</a><sup>1</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://sites.google.com/view/hongluzhou/" target="_blank">Honglu Zhou</a><sup>3</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.cs.rutgers.edu/people/professors/details/vladimir-pavlovic" target="_blank">Vladimir Pavlovic</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.cs.rutgers.edu/people/professors/details/mubbasir-kapadia" target="_blank">Mubbasir Kapadia</a><sup>4</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Rutgers University,</span>
                    <span class="author-block"><sup>2</sup>Amazon,</span>
                    <span class="author-block"><sup>3</sup>Salesforce AI Research,</span>
                    <span class="author-block"><sup>4</sup>Roblox</span>
                    <!-- <br> -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                    <br>
                    <span class="author-block"><b>ICML 2025</b></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://cjerry1243.github.io/casim_t2m/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://cjerry1243.github.io/casim_t2m/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/cjerry1243/casim_t2m" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://cjerry1243.github.io/casim_t2m/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
      <img src="static/images/TeaserImage.png" alt=""/>
      <p>
        <!-- <h2 class="subtitle has-text-centered"> -->
          <b>TLDR.</b> We propose CASIM, a composite-aware semantic injection mechanism for text to motion generation that exhibits stronger text-motion correspondence and better generalizability.
          </br>
          (Top) Fixed-length semantic injection, which primarily relied on the [CLS] token embedding from CLIP to represent the entire text prompt, fails to capture the subtle differences in individual words. As a result, it generates highly similar motions from distinct text prompts. 
          (Bottom) Our Composite aware semantic injection method allows each motion frame to dynamically attend to every word token (e.g., “left” or “right” hand), enhancing the motion-text correspondence.
        <!-- </h2> -->
      </p>
    </div>
  </div>
</section> 
<!-- End teaser -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in generative modeling and tok- enization have driven significant progress in text-to-motion generation, leading to enhanced quality and realism in generated motions. 
            However, effectively leveraging textual information for conditional motion generation remains an open challenge. 
            We observe that current approaches, primarily relying on fixed-length text embeddings (e.g., CLIP) for global semantic injection, struggle to capture the composite nature of human motion, resulting in suboptimal motion quality and controllability. 
            To address this limitation, we propose the Composite Aware Semantic Injection Mechanism (<b>CASIM</b>), comprising a composite aware text encoder and a text-motion aligner that learns the dynamic correspondence between text and motion tokens. 
            Notably, CASIM is model and representation-agnostic, readily integrating with both autoregressive and diffusion-based methods. 
            Experiments on HumanML3D and KIT bench- marks demonstrate that CASIM consistently improves motion quality, text-motion alignment, and retrieval scores across state-of-the-art methods. 
            Qualitative analyses further highlight the superiority of our composite aware approach over fixed-length semantic injection, enabling precise motion control from text prompts and stronger generalization to unseen text inputs.

          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Youtube video -->
<!-- <section class="section hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"></h2>
          <div class="content has-text-justified">
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/E652bX8U09w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Image carousel -->
<!-- <section class="section hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Synthetic Data Generation</h2>
          <div class="content has-text-justified">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <img src="static/images/DatasetComparison.png" alt=""/>
                <h2 class="subtitle has-text-centered">
                  A comparison of synthetic datasets as well as commonly-used real datasets for activity understanding and person tracking.
                </h2>
              </div>

              <div class="item">
                <img src="static/images/Pipeline.png" alt=""/>
                <h2 class="subtitle has-text-centered">
                  The data generation process of M<sup>3</sup>Act. It consists of multiple data simulations with scene instantiation, group activity authoring, 
                    and a data capture module. 
                  A high degree of randomization is involved in all aspects of the process to ensure diverse data.
                </h2>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End image carousel -->


<!-- Sample images -->
<!-- <section class="section hero is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Sample Images</h2>
          <div class="content has-text-justified">
            <div id="results-carousel" class="carousel results-carousel">

              <div class="item">
                <img src="static/images/RenderingCollage7.png" alt=""/>
                <h2 class="subtitle has-text-centered">
                </h2>
              </div>

              <div class="item">
                <img src="static/images/RenderingCollage6.png" alt=""/>
                <h2 class="subtitle has-text-centered">
                </h2>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- Sample images -->


<!-- Video carousel -->
<!-- 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> 
-->
<!-- End video carousel -->




<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
      <pre><code>
        <!-- @inproceedings{chang2024learning,
        title={Learning from Synthetic Human Group Activities},
        author={Chang, Che-Jui and Li, Danrui and Patel, Deep and Goel, Parth and Zhou, Honglu and Moon, Seonghyeon and Sohn, Samuel S and Yoon, Sejong and Pavlovic, Vladimir and Kapadia, Mubbasir},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={21922--21932},
        year={2024}} -->
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>